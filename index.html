<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>HeatherQian</title>
    <link rel="icon" href="img/favicon.ico" type="image/x-icon">
    <!-- Bootstrap -->
    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link href="ionicons/css/ionicons.min.css" rel="stylesheet">
    <link href="css/animate.min.css" rel="stylesheet">
    <link rel="stylesheet" href="css/animate.css">
    <link href="css/aos.css" rel="stylesheet">
    <link href="css/bootstrap.css" rel='stylesheet' type='text/css' />
    <link href="css/style.css" rel='stylesheet' type='text/css' />  
    <link rel="stylesheet" href="css/swipebox.css">
    <link href="css/font-awesome.min.css" rel='stylesheet' type='text/css' />   
    <!-- main style -->
    <link href="css/style.css" rel="stylesheet">
    
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
</head>

<body>

    <!-- Preloader -->
    <div id="preloader">
        <div id="status">

            <div class="preloader" aria-busy="true" aria-label="Loading, please wait." role="progressbar">
            </div>

        </div>
    </div>
    <!-- ./Preloader -->
    
    <!-- header -->
    <header class="navbar-fixed-top">
        <nav>
            <ul>
                <li><a href="#about">About</a></li>
                <li><a href="#experience">experience</a></li>
                <li><a href="#education">education</a></li>
                <li><a href="#skills">skills</a></li>
                <li><a href="#projects">projects</a></li>
                <li><a href="#contact">contact</a></li>
            </ul>
        </nav>
    </header>
    <!-- ./header -->
    
    <!-- home -->
    <div class="section" id="home" data-stellar-background-ratio="0.5">
        <div class="container">
            <div class="disply-table">
                <div class="table-cell" data-aos="fade-up" data-aos-delay="0">
                    <h4>Heather Qian</h4>
                    <h3>Bring Humanity to Products and Life</h3> </div>
            </div>
        </div>
    </div>
    <!-- ./home -->
    
    <!-- about -->
    <div class="section" id="about">
        <div class="container">
            <div class="col-md-6" data-aos="fade-up">
                <h4>01</h4>
                <h1 class="size-50">Know <br /> About me</h1>
                <div class="h-50"></div>
                <p>Hi, my name is Heather Qian. Wecomle to my website!</p> 
                <p>I have over 16 years of broad and professional experience in IT industry. 12 years of UX & UI design, product design & development & management. I’m excellent in strategic thinking and empirical research methodology with solid technical skills and big visions for the business.</p>
                <p>My passion is to design products that could help people and make their lives easier and better.</p>
                <p>I'm a gamer.</p>
                <div class="h-50"></div>
            </div>
            <div class="col-md-6 about-img-div">
                <div class="about-border" data-aos="fade-up" data-aos-delay=".5"></div>
                <img src="img/about-img.png" width="400" class="img-responsive" alt="" align="right" data-aos="fade-right" data-aos-delay="0" />
            </div>
        </div>
    </div>
    <!-- ./about -->
    
    <!-- experience  -->
    <div class="section" id="experience">
        <div class="container">
            <div class="col-md-12">
                <h4>02</h4>
                <h1 class="size-50">My <br /> Experience</h1>
                <div class="h-50"></div>
            </div>
            <div class="col-md-12">
                <ul class="timeline">
                    <li class="timeline-event" data-aos="fade-up">
                        <label class="timeline-event-icon"></label>
                        <div class="timeline-event-copy">
                            <p class="timeline-event-thumbnail">Sep 2018 - Sep 2020</p>
                            <h3>Instant Brands Inc.</h3>
                            <h4>Product Manager, UX & UI</h4>
                            <p><strong>Ottawa, Canada</strong>
                                <br>Lead and conducted user studies, deaign product user interface, interactions including wireframes, workflows, low/high-fidelity prototypes across all product lines, mobile apps and Voice UI by Illustrator, Balsamiq, Sketch, Axure RP, Principle, Keynote, etc.. Optimized research and design process, milestones, documents and standards for product R&D phases. Helped define product roadmap and milestones. Collaborated with factories in R&D phases. Mentored the product management team. Dug useful data and complied insightful reports from the huge data resources of Google Analytics. Used advanced statistical metrics to analyze data by Excel Macro, R, Python, Tableau and SPSS.</p>
                        </div>
                    </li>
                    <li class="timeline-event" data-aos="fade-up" data-aos-delay=".2">
                        <label class="timeline-event-icon"></label>
                        <div class="timeline-event-copy">
                            <p class="timeline-event-thumbnail">Apr 2015 - Present</p>
                            <h3>LQL Communication Technology</h3>
                            <h4>Co-founder / UX designer / UI Developer (Part-time)</h4>
                            <p><strong>Ottawa, Canada</strong>
                            <br>Collected major pain points from clients, conducted ﬁeld trips, observations, interviews for gathering and exploring entire and hidden requirements, developed the user persona, scenarios and use cases. Designed user interface, workﬂows, wireframes, low-ﬁdelity and high-ﬁdelity prototypes by Balsamiq, Sketch and Axure RP. Developed UI and front-end based on Microsoft .net UI tools such as DevExpress. Worked with hardware and software engineers in development and testing stages. Usability testing on different iterations.</p>
                        </div>
                    </li>
                    <li class="timeline-event" data-aos="fade-up" data-aos-delay=".4">
                        <label class="timeline-event-icon"></label>
                        <div class="timeline-event-copy">
                            <p class="timeline-event-thumbnail">Sep 2016 - Present</p>
                            <h3>Carleton University</h3>
                            <h4>Teaching Assistant (Part-time)</h4>
                            <p><strong>Ottawa, Canada</strong>
                                <br>Help the teaching activies in multiple Courses: Programming in Python, Interaction Design, Statistics and R, Advanced Game Design and Development, etc. </p>
                        </div>
                    </li>
                    <li class="timeline-event" data-aos="fade-up" data-aos-delay=".6">
                        <label class="timeline-event-icon"></label>
                        <div class="timeline-event-copy">
                            <p class="timeline-event-thumbnail">Sep 2016 - Present</p>
                            <h3>Carleton University</h3>
                            <h4>Research Assistant (Part-time)</h4>
                            <p><strong>Ottawa, Canada</strong>
                                <br>Conducted experimental design, software and hardware implementation and testing for publications. </p>
                        </div>
                    </li>
                    <li class="timeline-event" data-aos="fade-up" data-aos-delay=".8">
                        <label class="timeline-event-icon"></label>
                        <div class="timeline-event-copy">
                            <p class="timeline-event-thumbnail">Nov 2014 - Mar 2015</p>
                            <h3>Somoplay Inc.</h3>
                            <h4>UX/UI Designer</h4>
                            <p><strong>Toronto, Canada</strong>
                                <br>Product strategy for mobile applications. Information architecture and Interactive design of mobile applications. Designed Low-ﬁdelity and high-ﬁdelity prototypes.</p>
                        </div>
                    </li>
                    <li class="timeline-event" data-aos="fade-up" data-aos-delay=".10">
                        <label class="timeline-event-icon"></label>
                        <div class="timeline-event-copy">
                            <p class="timeline-event-thumbnail">Feb 2010 - Aug 2013</p>
                            <h3>ZTE Corporation</h3>
                            <h4>Product Manager</h4>
                            <p><strong>Worldwide Ofﬁces (mostly in USA, Japan and Europe)</strong>
                                <br>Researched vendor’s technology, schedule and platform evolutions, developed strategies of product lines and families, roadmaps, time schedule and milestones for software and hardware products. Played different roles in different research & development life cycles with both waterfall and agile developments (always in parallel projects and multiple tasks). Researched and analyzed user and customized speciﬁcally requirements, developed weekly reports to the stakeholders, conducted and drove customer requirements meetings with internal and external stakeholders. Marketing research and analysis, benchmarking research and analysis, trend research and analysis with strong business acumen, write weekly/monthly trend and competitive intelligence analysis(CI) reports. Developed and improved on process best practices, use cases, document templates for the purpose of designing, analyzing, testing and improving products.</p>
                        </div>
                    </li>
                    <li class="timeline-event" data-aos="fade-up" data-aos-delay=".12">
                        <label class="timeline-event-icon"></label>
                        <div class="timeline-event-copy">
                            <p class="timeline-event-thumbnail">Mar 2008 - Feb 2010</p>
                            <h3>ZTE Corporation</h3>
                            <h4>UX Designer</h4>
                            <p><strong>China</strong>
                                <br>Conducted qualitative and quantitative user research such as interviews, surveys, dairy study, field research/visits, eye-tracking and card sorting. Designed interfaces and interactions for mobile phones, tablets, CPE and USB modems. Conducted usability and A/B testings, and tracked the project in the entire product life cycle. </p>
                        </div>
                    </li>
                    <li class="timeline-event" data-aos="fade-up" data-aos-delay=".14">
                        <label class="timeline-event-icon"></label>
                        <div class="timeline-event-copy">
                            <p class="timeline-event-thumbnail">Jul 2004 - Mar 2008</p>
                            <h3>China Mobile</h3>
                            <h4>Operation and Support Engineer</h4>
                            <p><strong>China</strong>
                                <br>Supported the network with Ericsson, Huawei, ZTE equipment, data network management systems, protocol analysis equipment. Analyzed alarms and logs, different levels of incidents and system reports by troubleshooting, and repaired the software and hardware problems.</p>
                        </div>
                    </li>
                </ul>
            </div>
        </div>
    </div>
    <!-- ./experience -->

<!-- education  -->
    <div class="section" id="education">
        <div class="container">
            <div class="col-md-12">
                <h4>03</h4>
                <h1 class="size-50">My <br /> Education</h1>
                <div class="h-50"></div>
            </div>
            <div class="col-md-12">
                <ul class="timeline">
                    <li class="timeline-event" data-aos="fade-up">
                        <label class="timeline-event-icon"></label>
                        <div class="timeline-event-copy">
                            <p class="timeline-event-thumbnail">Sep 2019 - Present</p>
                            <h3>Carleton University</h3>
                            <h4>PhD in Information Technology</h4>
                            <p><strong>Ottawa, Canada</strong>
                                <br>Research ﬁelds: Empirical research on human and computer interactions, Virtual Reality and Mixed Reality, Machine Learning, Deep Learning, Big Data, and AI applications in VR and AR.</p>
                        </div>
                    </li>
                    <li class="timeline-event" data-aos="fade-up" data-aos-delay=".2">
                        <label class="timeline-event-icon"></label>
                        <div class="timeline-event-copy">
                            <p class="timeline-event-thumbnail">Sep 2016 - Jun 2018</p>
                            <h3>Carleton University</h3>
                            <h4>Master of Computer Science in Human-Computer Interaction (HCI)</h4>
                            <p><strong>Ottawa, Canada</strong>
                            <br>Major courses: Fundamentals of HCI design and evaluation, Software and user interface development, Empirical research perspective, Emerging interaction techniques. Research ﬁelds: Empirical research on human and computer interactions, Virtual Reality and Mixed Reality. Accomplished 5 publications as the first author.</p>
                        </div>
                    </li>
                    <li class="timeline-event" data-aos="fade-up" data-aos-delay=".4">
                        <label class="timeline-event-icon"></label>
                        <div class="timeline-event-copy">
                            <p class="timeline-event-thumbnail">Sep 2000 - Jul 2004</p>
                            <h3>Nanjing Normal University</h3>
                            <h4>Bachelor in Electronics and Computer Engineering</h4>
                            <p><strong>Nanjing, China</strong>
                                <br>Double Major in telecommunications and software engineering.</p>
                        </div>
                    </li>
                        </div>
                    </li>
                </ul>
            </div>
        </div>
    </div>
    <!-- ./education -->

<!--skills-->
    <div class="section" id="skills">
        <div class="container">
            <div class="col-md-12">
                <h4>04</h4>
                <h1 class="size-50">My <br /> Skills</h1>
                <div class="h-50"></div>
            </div>
            <div class="col-md-6 skills-left">
                <h3><strong>UX & UI Design</strong></h3>
                <p>UX&UI design relies on various user research tailored by stakeholders' requirements and project milestones. To reach this goal, I designed, led, and conducted <strong>qualitative</strong>, <strong>quantitative</strong>, and even <strong>empirical</strong> research in different stages.</p>
                <p>This chart represents the design tools I used recently. However, other tools such as Microsoft Visio, Adobe XD, Figma, Adobe After Effects, Zeplin and Principle have all been used as alternatives based on what the teams were comfortable with.</p>   
            </div>
            <div class="col-md-6 skills-right">
                <div class="bar_group">
                    <div class='bar_group__bar thin' label='Adobe Illustrator' show_values='true' tooltip='true' value='90'></div>
                    <div class='bar_group__bar thin' label='Balsamiq' show_values='true' tooltip='true' value='100'></div>
                    <div class='bar_group__bar thin' label='Sketch' show_values='true' tooltip='true' value='90'></div>
                    <div class='bar_group__bar thin' label='InVision' show_values='true' tooltip='true' value='95'></div>
                    <div class='bar_group__bar thin' label='Axure RP' show_values='true' tooltip='true' value='95'></div>
                </div>
                
            </div>

            <div class="col-md-6 skills-left">
                <h3><strong>Programming</strong></h3>
                <p>I like to use Python to conduct data analysis and visualizations with IDEs and Libraries like Anaconda, Jupyter Notebook, Pandas, TensorFlow, Matplotli, D3.js, etc.</p>
                <p>I use C# and Unity 3D to develop 3D/VR/AR applications and games including academic and commercial projects.</p>   
                <p>Other programming languages such as C, Visual Basics (VBA in Excel), and R were also used as alternatives based on the development requirements.</p>   
            </div>
            <div class="col-md-6 skills-right">
                <div class="bar_group">
                    <div class='bar_group__bar thin' label='Python (and Anaconda, Jupyter Notebook, etc. )' show_values='true' tooltip='true' value='90'></div>
                    <div class='bar_group__bar thin' label='C# (with Unity 3D)' show_values='true' tooltip='true' value='90'></div>
                    <div class='bar_group__bar thin' label='HTML' show_values='true' tooltip='true' value='100'></div>
                    <div class='bar_group__bar thin' label='CSS' show_values='true' tooltip='true' value='85'></div>
                    <div class='bar_group__bar thin' label='Javascript' show_values='true' tooltip='true' value='70'></div>
                </div>
                
            </div>

            <div class="clearfix"></div>
            
    </div>
</div>
<!--/skills-->






    <!-- projects -->
    <div class="section" id="projects">
        <div class="container">
            <div class="col-md-12">
                <h4>05</h4>
                <h1 class="size-50">My <br /> Projects</h1> 
            </div>
            <!-- main container -->
            <div class="main-container portfolio-inner clearfix">
                <!-- portfolio div -->
                <div class="portfolio-div">
                    <div class="portfolio">
                        <!-- portfolio_filter -->
                        <div class="categories-grid wow fadeInLeft">
                            <nav class="categories">
                                <ul class="portfolio_filter">
                                    <li><a href="" class="active" data-filter="*">All</a></li>
                                    <li><a href="" data-filter=".mobileapp">Mobile App</a></li>
                                    <li><a href="" data-filter=".iot">IoT</a></li>
                                    <li><a href="" data-filter=".vrar">VR/AR</a></li>
                                    <li><a href="" data-filter=".datavis">Data Visualization</a></li>
                                    <li><a href="" data-filter=".usability">Usability Testing</a></li>
                                    <li><a href="" data-filter=".process">Design Process</a></li>
                                    <li><a href="" data-filter=".empirical">Empirical Study</a></li>
                                    <li><a href="" data-filter=".deeplearning">AI</a></li>
                                </ul>
                            </nav>
                        </div>
                        <!-- portfolio_filter -->
                        
                        <!-- portfolio_container -->
                        <div class="no-padding portfolio_container clearfix" data-aos="fade-up">
                            <!-- single work -->
                            <div class="col-md-4 col-sm-6  mobileapp usability">
                                <a id="demo01" href="#animatedModal" class="portfolio_item"> <img src="img/portfolio/redesign01.jpg" alt="image" class="img-responsive" />
                                    <div class="portfolio_item_hover">
                                        <div class="portfolio-border clearfix">
                                            <div class="item_info"> <span>Redesign a Recipe App</span> <em>2018-2019</em> </div>
                                        </div>
                                    </div>
                                </a>
                            </div>
                            <!-- end single work -->
                            
                            <!-- single work -->
                            <div class="col-md-4 col-sm-6 datavis process">
                                <a id="demo02" href="#animatedModal02" class="portfolio_item"> <img src="img/portfolio/datavis.jpg" alt="image" class="img-responsive" />
                                    <div class="portfolio_item_hover">
                                        <div class="portfolio-border clearfix">
                                            <div class="item_info"> <span>Design Data Visualization for a Reading Log Application</span> <em>2020</em> </div>
                                        </div>
                                    </div>
                                </a>
                            </div>
                            <!-- end single work -->
                            
                            <!-- single work -->
                            <div class="col-md-4 col-sm-6 iot process empirical">
                                <a id="demo03" href="#animatedModal03" class="portfolio_item"> <img src="img/portfolio/voice.jpg" alt="image" class="img-responsive" />
                                    <div class="portfolio_item_hover">
                                        <div class="portfolio-border clearfix">
                                            <div class="item_info"> <span>Design Voice UI for a Senoir's Painting App</span> <em>2019</em> </div>
                                        </div>
                                    </div>
                                </a>
                            </div>
                            <!-- end single work -->
                            
                            <!-- single work -->
                            <div class="col-md-4 col-sm-6 deeplearning">
                                <a id="demo04" href="#animatedModal04" class="portfolio_item"> <img src="img/portfolio/ai01.jpg" alt="image" class="img-responsive" />
                                    <div class="portfolio_item_hover">
                                        <div class="portfolio-border clearfix">
                                            <div class="item_info"> <span>Sentiment Analysis of Products on Amazon by Using Deep Learnings</span> <em>2020</em> </div>
                                        </div>
                                    </div>
                                </a>
                            </div>
                            <!-- end single work -->
                            
                            <!-- single work -->
                            <div class="col-md-4 col-sm-6 vrar empirical">
                                <a id="demo05" href="#animatedModal05" class="portfolio_item"> <img src="img/portfolio/vr01.jpg" alt="image" class="img-responsive" />
                                    <div class="portfolio_item_hover">
                                        <div class="portfolio-border clearfix">
                                            <div class="item_info"> <span>Eye-Based Selection and Navigation in Virtual Reality</span> <em>2018</em> </div>
                                        </div>
                                    </div>
                                </a>
                            </div>
                            <!-- end single work -->
                            
                            <!-- single work -->
                            <div class="col-md-4 col-sm-6 iot empirical">
                                <a id="demo06" href="#animatedModal06" class="portfolio_item"> <img src="img/portfolio/helmet.jpg" alt="image" class="img-responsive" />
                                    <div class="portfolio_item_hover">
                                        <div class="portfolio-border clearfix">
                                            <div class="item_info"> <span>Safety Helmet and Wristbands for Cyclists</span> <em>2018</em> </div>
                                        </div>
                                    </div>
                                </a>
                            </div>
                            <!-- end single work -->
                        </div>
                        <!-- end portfolio_container -->
                    </div>
                    <!-- portfolio -->
                </div>
                <!-- end portfolio div -->
            </div>
            <!-- end main container -->
        </div>
    </div>
    <!-- ./projects -->
    
    <!-- contact -->
    <div class="section" id="contact">
        <div class="container">
            <div class="col-md-12">
                <h4>06</h4>
                <h1 class="size-50">Contact  Me</h1>
                <div class="h-50"></div>
            </div>
            <div class="col-md-4" data-aos="fade-up">


                <h3>Email</h3>
                <p>heather.qian@hotmail.com </p>

                <div class="clearfix"></div>
                Copyright &copy; 2017-2020 Heather Qian. All rights reserved.
                <div class="h-50"></div>
            </div>
            <div class="col-md-8" data-aos="fade-up">
                <form class="contact-bg" id="contact-form" name="contact" method="post" novalidate>
                    <input type="text" name="name" class="form-control" placeholder="Your Name" />
                    <input type="email" name="email" class="form-control" placeholder="Your E-mail" />
                    <input type="text" name="phone" class="form-control" placeholder="Phone Number" />
                    <textarea name="message" class="form-control" placeholder="Your Message" style="height:120px"></textarea>
                    <button id="submit" type="submit" name="submit" class="btn btn-glance">Send</button>
                    <div id="success">
                        <p class="green textcenter"> Your message was sent successfully! I will be in touch as soon as I can. </p>
                    </div>
                    <div id="error">
                        <p> Something went wrong, try refreshing and submitting the form again. </p>
                    </div>
                </form>
            </div>
        </div>
    </div>
    <!-- ./contact -->

   <!--DEMO01-->
    <div id="animatedModal" class="popup-modal">
        <!--THIS IS IMPORTANT! to close the modal, the class name has to match the name given on the ID -->
        <div id="btn-close-modal" class="close-animatedModal close-popup-modal"> <i class="ion-close-round"></i> </div>
        <div class="clearfix"></div>
        <div class="modal-content">
            <div class="container">
                <div class="portfolio-padding">
                    <div class="col-md-8 col-md-offset-2">
                        <h2>Redesign a Recipe App<br /></h2>
                        <h3>Mobile App & Usability Testing<br /></h3>
                        <div class="h-50"></div>
                        <p><strong>Introduction of the App</strong> </p>
                        <p>This recipe app was designed for helping end users find and follow recipes when using a specific cooking appliance. All the recipes were customized to the cooking appliance and could be remotely controlled by the app and voice via Google and Amazon IoT speakers as the future features. The goal of this redesign project was to find all usability issues, make changes thus get better prepared for further development of more features. </p>
                        <br />
                        <p><strong>My Role</strong> </p>
                        <p>I was responsible for the entire redesign project including planning and executing proper evaluations, analyzing the results, compiling the reports and redesigning the UIs. I had an assistant to record the video and take notes in the usability testing. </p>
                        <br />
                        <p><strong>The Process</strong> </p>
                        <p>The process was divided into 2 main parts, usability testing and UI improvements. To locate the current issues, a <strong>usability testing</strong> and a post-test <strong>interview</strong> were conducted. I recruited 10 participants from the local community who had different levels of experience of the appliance product and the app.Usability issues were found and addressed. In the UI improvements stage,  some small user studies such as <strong>card sorting, wizard of Oz and A/B tests</strong> were conducted to validate the results of the usability testing and UI design directions.</p>
                        <br />
                        <p><strong>Usability Testing</strong> </p>
                        <p>4 major modules of the app were testing that covered 9 scenarios and 24 tasks, and evaluated by several metrics such as success/error rates, task time, Jacob Nielsen’s 10 heuristics, system usability scale (SUS), Whitney Quesenbery’s 5Es, severity rankings, etc. </p>
                        <br /> <img src="img/portfolio/redesign02.jpg" alt="" class="img-responsive" />
                        <br /> <img src="img/portfolio/redesign03.jpg" alt="" class="img-responsive" />
                        <br /> <img src="img/portfolio/redesign04.jpg" alt="" class="img-responsive" />
                        <br />
                        <p>According to Nielsen’s heuristics, I found that the below aspects were rated in lower scores: </p>
                        <ul>
                        <li>User control and freedom</li>
                        <li>Recognition rather than recall</li>
                        <li>Help users recognize, diagnose, and recover from errors</li>
                        </ul>
                        <br />
                        <p>From locating the issues based on each screen, there were 4 major objectives for the redesign: </p>
                        <ol>
                        <li>Simplify the information architecture</li>
                        <li>Unify screen elements including colors, fonts and icons</li>
                        <li>Refine the wording</li>
                        <li>Simplify the interactions</li>
                        <li>Make the workflow smoother and clearer</li>
                        </ol>
						<br />
                        <p><strong>Here're a few examples.</strong></p>
                        <p><strong>1. Color Design for text labels</strong></p>
                        <p>In the app, there were 3 different gray colours that all represented the minor information of texts and they were clickable. This inconsistency confused the users that the darker text might be clickable but the lighter text was unclickable. </p>
                        <img src="img/portfolio/redesign05.jpg" alt="" class="img-responsive" />
                        <br /> 
                        <p><strong>Solution </strong></p>
                        <p>These 3 different colors should be unified into one color. I chose the darker gray to distinguish from unclickable gray color. </p>
                        <img src="img/portfolio/redesign06.jpg" alt="" class="img-responsive" />
                        <br /> 
                        <br />
						<p><strong>2. Home Screen's Architecture and Wording</strong></p>
                        <p>In the usability testing, all the participants found it was difficult to navigate on the home screen: the user guide section was split into 2 parts for reinforcing the branding value and was entitled with the company's name according to the needs of marketing; the different sections of recipes were not well identified and named, participants didn't understand the "featured recipe" and didn't show any interests on the "newest recipe". </p>
                        <br /> 
						<p><strong>Solution </strong></p>
						<p>Redesigned the layout of the home screen after a few iterations and A/B tests. See below.</p>
						<img src="img/portfolio/redesign08.jpg" alt="" class="img-responsive" />
						<br />
                        <br />
                        <p><strong>3. Icon and Its Interactions </strong></p>
                        <p>There were 3 different colors of checkmarks represented different meanings in the App. In the usability testing, all the participants were very confused when multiple items were selected, especilly after adding a few items to the "Grocery" list, 3 different colors of checkmarks were all displayed on the screen. Users were hesitated to select items. They could not undo the operations correctly and foresee the potencial errors.  </p>
                        <img src="img/portfolio/redesign07.jpg" alt="" class="img-responsive" />
                        <br /> 
                        <br /> 
						<p><strong>Solution </strong></p>
						<p>To make the screen simpler and clearer, I removed all the gray checkmarks that reprents "not checked item". To distinguish the confusion of two different interactions "check" and "add to the grocery", I split them into 2 different places and icons. A shopping cart icon was added to represent the current numbers of the items on the grocery list.  </p>
						<img src="img/portfolio/redesign10.jpg" alt="" class="img-responsive" />
						<br /> 
						<p>See the comparion below. </p>
						<img src="img/portfolio/colorbefore.gif" alt="" class="img-responsive" />
                        <img src="img/portfolio/colorafter.gif" alt="" class="img-responsive" />
                        <br />
                        <br />
						<p><strong>Result</strong></p>
                        <p>After a few iterations of the redesign, the SUS(system usability scale) score was greatly increased from 39.5% to 76%.</p>
                        <img src="img/portfolio/redesign09.jpg" alt="" class="img-responsive" />
                        <br /> 
                        <br /> </div>
                </div>
            </div>
        </div>
    </div>
 
    <!--DEMO02-->
    <div id="animatedModal02" class="popup-modal">
        <!--THIS IS IMPORTANT! to close the modal, the class name has to match the name given on the ID -->
        <div id="btn-close-modal" class="close-animatedModal02 close-popup-modal"> <i class="ion-close-round"></i> </div>
        <div class="clearfix"></div>
        <div class="modal-content">
            <div class="container">
                <div class="portfolio-padding">
                    <div class="col-md-8 col-md-offset-2">
                        <h2>Design Data Visualization for a Reading Log Application<br /></h2>
                        <h3>Data Visualization & Design Process<br /></h3>
                        <div class="h-50"></div>
						<p><strong>Introduction/Background</strong> </p>
                        <p>This data visualization has been designed for being embedded in a SaaS system to show users' reading logs. The reading log was a part of personal management system to self-manage user's time and resources for both screens and off-line activities. The objectives of this project was to design a novel visualization of several types of reading activites that should be different from the traditional data charts.</p>
                        <br />
                        <p><strong>My Role</strong> </p>
                        <p>I was responsible for the entire design project, deliver the high-fidelity prototype and collaborate the software development. </p>
                        <br />
                        <p><strong>The Requirements and Constrains</strong> </p>
                        <p>The stakeholders requirements were to visualize the reading sources/types and sentiment based on the time, describe the user's reading routine easily in a fresh manner. The data should be summarized and displayed in periods such as day, week, month and/or year. The reading sources and sentiment would be categorized into segments in the furture (not the first version). The major requirements are listed below:</p>
                        <ol>
                        <li>Present the time.</li>
                        <li>Present the reason and the source of users' reading based on user's inputs.</li>
                        <li>Present the sentiment of each reading based on user's inputs.</li>
                        <li>Allow the filtering based on the time.</li>
                        <li>Allow the filtering based on the reading source.</li>
                        <li>Allow the filtering based on the sentiment.</li>
                        <li>The primary time window could be a day or a week.</li>
                        </ol>
                        <br />
                        <p>There were also a few constrains from the requirements. </p>
                        <ol>
                        <li>Do not use any traditional data charts that could be derived from Excel, Tableau, etc.</li>
                        <li>The visualization would be embedded into part of SaaS dashboard, the space for the visualization was limited.</li>
                        <li>Need to consider potencial design features for acommodating more detailed features in the future.</li>
                        </ol>
                        <br /> 
                        <p><strong>The Process</strong> </p>
                        <img src="img/portfolio/datavis01.jpg" alt="" class="img-responsive" />
                        <br />
                        <p><strong>Low-fidelity Prototype</strong> </p>
                        <p>Designing data visualization is similar to app design that should follow some guidelines and stardards. For example, the first step was to identify data types, marks, channels and visual encodings.</p>
                        <img src="img/portfolio/datavis02.jpg" alt="" class="img-responsive" />
                        <br />
                        <p>Some low-fidelity prototypes by paper and pen</p>
                        <img src="img/portfolio/datavis03.jpg" alt="" class="img-responsive" />
                        <br />
                        <br />
                        <p>A High-fidelity prototype based on above sketches and user testing (A/B tests, interviews and surveys)</p>
                        <img src="img/portfolio/datavis.gif" alt="" class="img-responsive" />
                        <br />
                        <p><strong>Stay tuned. More details will be revealed soon...</strong> </p>
                        <br /> </div>
                </div>
            </div>
        </div>
    </div>

    <!--DEMO03-->
    <div id="animatedModal03" class="popup-modal">
        <!--THIS IS IMPORTANT! to close the modal, the class name has to match the name given on the ID -->
        <div id="btn-close-modal" class="close-animatedModal03 close-popup-modal"> <i class="ion-close-round"></i> </div>
        <div class="clearfix"></div>
        <div class="modal-content">
            <div class="container">
                <div class="portfolio-padding">
                    <div class="col-md-8 col-md-offset-2">
                        <h2>Design Voice UI for a Senoir's Painting App<br /></h2>
                        <h3>IoT & Design Process & Empirical Study<br /></h3>
                        <div class="h-50"></div>
                        <p><strong>My Role</strong> </p>
                        <p>This was a consulting project of a future painting app using voice recognition technology. The project's goal was to investigate and evaluate the feasibility of the app. The app was defined as a therapy app for seniors according to the skateholders' initial requirements. I was responsible to identify the user needs and design the effective features, functionalities, architecture, interactions, and future user study and evaluations for UI design and software development. </p>
                        <br />
                        <p><strong>Introduction</strong> </p>
                        <p>Nowadays, drawing and painting is highly recommended to seniors as an art therapy. Seniors living with a number of ailments may beneﬁt from art therapy. This is a targeted use of painting to help improve the memory, reduce stress and alleviate pain in a setting that is fun, casual and social. Some researches show that painting, drawing and sculpting in old age may protect against dementia. Meanwhile, after working hard for the family for some decades, many seniors would like to look for a long term hobby that they might want to pursue in their younger ages but give up due to busy and fast-paced working for their careers and money. However, there are many constrains against painting by seniors due to their health problems. First, to hold the drawing pencil, it needs the ﬁngers bend their joint bones properly to draw accurately. To grasp the painting brush, it requires the muscles support the arm in the mid air and work together with the wrist and ﬁngers. The seniors may not be able to hold the brush for a long time because of muscle atrophy or arthritis. Many painting stands are designed for standing upright, they might also be very large that requires the artist to bend the waist and knees. At this moment, the seniors would not be able to perform such actions, especially for those with mobility issues. Additionally, the painting activity also requires the artist to be very concentrated and remember her initial creative idea since the painting work would last for many times or days. Therefore, although many seniors have the needs on drawing for different purposes, the painting activities require very much efforts on mental and physical health conditions.</p>
                        <p>As the result, the regular means of painting activities by hands could not fulﬁll the special needs from senior citizens. We know that some artists with disability utilized mouth or foot to paint, but that cannot solve the problem for seniors because they would also cause very high physical fatigue, besides, mouth or foot painting needs training. And some neurofeedback technologies, for example, using EGG signals [1, 2] to paint requires expensive devices, highly trained researcher and users, and technical contains in terms of accuracy, which cannot be widely used for general public. Inspired by the neurofeedback technologies, voice that can perform orders from the mouth directly, is a more reliable way to control painting than EGG signals. Voice has been utilized as an alternative to control IoT products like smart lights, bulbs, TVs, etc. since the past few years. By speaking out the orders and actions that the seniors want, then convert the orders into output devices to complete the tasks, it needs much less physical efforts and possibly less mental focus if is is designed properly. The seniors have no need to stand, walk, lift the arms or even bend the ﬁngers. Thus, from connecting the needs of painting and problems of seniors, the voice recognition would be a solution for solving them.</p>
                        <br /> <img src="img/portfolio/voice2.jpg" alt="" class="img-responsive" />
                        <br />
                        <br />
                        <p><strong>Technology Affordance</strong> </p>
                        <p>Back in 1988, Froessl [3] invented a method to manipulate image by speech signals. In 1989, Porter et al. [4] invented a more explicit voice recognition system based on previous inventions about a a text locating system recognizes spoken utterances, uses the recognized words as a search string, and searches text for words matching that search string. In early 21 centuries, Wright et al. [5] utilized spoken commands to receive data and manipulate the actions on computers. Then, many other researchers like Erell et al. [6] optimized the voice recognition technology like synthesized speech fragment for independent input devices. Empirical studies [7] started to investigate the efﬁciency between voice and other modalities like hand-writing and type-writing in 1970s. Cohen et al. [8] compared naturual language interaction with GUI and direct manipulation and found that applications requiring speedy user input of complex descriptions favoured spoken natural language communication. Thanks to these pioneer studies, many applications about voice recognition have been developed and evaluated since then. Igarashi et al. [9] proposed an application about using nonverbal voice input like pitch and volume for directing control interactions. Then, Perera et al. [10] reported their experiments and results to use sound volume and a few sound patterns to control the drawing for upper limb disabilities. They believed that paralinguistic voice can assisting artists with upper limb disabilities to perform tasks like creating visual art. Based on the academical accomplishments, the industry started to develop voice assistant applications from Apple Siri, to Amazon Alexa and Google. Especially for Alexa, which was just released in 2014, announced in April 2019, Amazon had over 90,000 functions ("skills") available for users to download on their Alexa-enabled devices. As we have already known from the literatures about the mechanism of how the voice recognition system process inputs and outputs. Let us take an example of Alexa to see how voice was converted to actual applications. In Amazon Alexa, a skill is an entire function that includes intents, slots, and utterances.</p>
                        <br /> <img src="img/portfolio/voice3.jpg" alt="" class="img-responsive" />
                        <br />
                        <p>An intent represents an action that fulﬁlls a user's spoken request. Intents can optionally have arguments called slots. The sample utterances are a set of likely spoken phrases mapped to the intents. When you create a new custom intent, you provide a name and a list of utterances that users would say to invoke this intent. Then you identify slots for each intents. A skill can have many intents as user actions like add, delete, select, paint, erase, etc. An intent also can have many slots like the time, length, number, etc.</p>
                        <br /> <img src="img/portfolio/voice4.jpg" alt="" class="img-responsive" /> <img src="img/portfolio/voice5.jpg" alt="" class="img-responsive" />
                        <br />
                        <p>In addition to building the skills, Alexa has utilized AI to learn natural language understanding, natural language generation, context modelling, common-sense reasoning and dialog planning.</p>
                        <br />
                        <p><strong>Initial Design</strong> </p>
                        <p>The picture below shows the system architecture of the painting application by voice recognition with Amazon Alexa. The reason I chose to use Alexa because it has been a very mature ecosystem that provides many output devices and Amazon Web Service (AWS) including the cloud. The output devices can be a tablet, a computer, a smart TV or even motor-based mechanical arms that can draw pictures on more traditional materials like paper according to users’ needs. Regarding to the possible issues of visual impairment, I added an extra input, that is a camera that can assist the voice input to amend the orders when the seniors are copy painting or natural sketching. The image processing could allow the users to learn the dimensions of objects and understand better the picture structure. All the data will be sent to Amazon cloud for storing and machine learning.</p>
                        <br /> <img src="img/portfolio/voice6.jpg" alt="" class="img-responsive" /> 
                        <br />
                        <p>Before designing the Alexa skills, we should understand how to describe an object in painting language. For example, the basic elements in painting are dots, lines and shapes. When we draw a dot, we would ﬁrst locate it on the canvas, then decide the size (or the diameter) of the dot, at last we would decide the color. If drawing a line, we would decide the beginning and the end of the line, the degrees or the slope, the length, then the color. And when drawing the shapes, we would select among a circle, an oval, a rectangle, a square, a star, etc., then the diameter, the color of the border and the inner part, and even the shadow, transparency, reﬂection patterns. After ﬁnishing the structure of the picture, we would decide how to color the entire picture and make reﬁnement by combining some shapes, erasing some lines, etc. Therefore, this Alexa Skill named “Painting Skill” could have several intents like “draw”, “ﬁll color”, “delete”, “erase”, “expand”, etc. Each intent might have several slots like element types, locations or coordinates, size, diameter, and patterns. The element types may have parameters like “line”, “circle”, “rectangle”, “star”, etc. We can also add some templates for several common objects like trees, ﬂowers, birds, etc. and provide a pattern source of database for each template.</p>
                        <br /> <img src="img/portfolio/voice7.jpg" alt="" class="img-responsive" /> 
                        <br />
                        <p>When considering the real users, we need to divide the user needs based on some segments. For instance, the seniors citizens would have different levels of painting skills regarding to novice and experienced users. They might also want to learn or utilize different levels of difﬁculties of painting skills depending on the scenarios. For example, if a novice senior who has never painted before, she might ask Alexa to put the objects by words “left, right or top”, then ask the Alexa to move left or up to make changes. However, the experienced users may have more accurate control on the painting brush or the cursor, and might be able to use four quadrants to locate.</p>
                        <br /> <img src="img/portfolio/voice8.jpg" alt="" class="img-responsive" /> 
                        <br />
                        <p>For the seniors who have visual impairment, the computer vision from the camera could tell them the general dimensions and locations of the objects they want to paint, or take pictures of what they have been drawing in process thus tell them what they have so far.</p>
                        <br />
                        <p><strong>Design experiment and evaluations</strong> </p>
                        <p>Before designing the explicit low and high-fidelity prototypes, we need to assess if this initial design can solve the problem at the beginning. We could evaluate from the objective and subjective point of view as listed below:</p>
                        <ol>
                        <li>What is the performance of voice recognition on painting for senior?</li>
                        <li>What is the satisfaction, immersion and frustration?</li>
                        </ol>
                        <br />
                        <p>Experimental design</p>
                        <p><u>Independent variables:</u></p>
                        <p>Inputs: <i>voice drawing, hand-drawing, computer graphic drawing.</i></p>
                        <p><u>Dependent variables:</u></p>
                        <p>Objective: <i>Task completion time, errors made in the process like the usages of “erase” and “delete”, heart rate before and after, blood pressure before and after.</i></p>
                        <p>Subjective: <i>SUS questionnaire to measure usability, learnability and consistency, satisfaction and fatigues questionnaire.</i></p>
                        <br/>
                        <p>We should also conduct a longitudinal study to evaluate the changes the participants made after repeating the sessions in a long term. The seniors have longer learning curve because they have worse memories and slower reactions than adults so a longitudinal study could be better to understand the system effectiveness.</p>
                        <br />
                        <br />
                        <br />
                        <p><strong>References</strong> </p>
                        <ol>
                        <li>Münßinger, J. I., Halder, S., Kleih, S. C., Furdea, A., Raco, V., Hösle, A., & Kubler, A. (2010). Brain painting: ﬁrst evaluation of a new brain–computer interface application with ALS-patients and healthy volunteers. Frontiers in neuroscience, 4, 182.</li>
                        <li>Zickler, C., Halder, S., Kleih, S. C., Herbert, C., & Kübler, A. (2013). Brain painting: usability testing according to the user-centered design in end users with severe motor paralysis. Artiﬁcial intelligence in medicine, 59(2), 99-110.</li>
                        <li>Froessl, H. (1988). U.S. Patent No. 4,726,065. Washington, DC: U.S. Patent and Trademark Ofﬁce.</li>
                        <li>Porter, E. W. (1989). U.S. Patent No. 4,829,576. Washington, DC: U.S. Patent and Trademark Ofﬁce.</li>
                        <li>Wright, B. D., Dubach, J., Parmenter, D. W., Gold, A., Young, J. H., & Gould, J. M. (2003). U.S. Patent No. 6,601,027. Washington, DC: U.S. Patent and Trademark Ofﬁce.</li>
                        <li>Erell, A., & Melzer, E. (2005). U.S. Patent Application No. 10/857,848.</li>
                        <li>Ochsman,R.B.&Chapanis,A.(1974)Int.J.Man-Mach.Stud. 6,579-620.</li>
                        <li>Cohen, P. R., & Oviatt, S. L. (1995). The role of voice input for human-machine communication. proceedings of the National Academy of Sciences, 92(22), 9921-9927.</li>
                        <li>Igarashi, T., & Hughes, J. F. (2001, November). Voice as sound: using non-verbal voice input for interactive control. In Proceedings of the 14th annual ACM symposium on User interface software and technology (pp. 155-156). ACM.</li>
                        <li>Perera, D., Eales, R. J., & Blashki, K. (2009). Supporting the creative drive: investigating paralinguistic voice as a mode of interaction for artists with upper limb disabilities. Universal Access in the Information Society, 8(2), 77-88.</li>
                        </ol>
                        <br /> </div>
                </div>
            </div>
        </div>
    </div>

<!--DEMO04-->
    <div id="animatedModal04" class="popup-modal">
        <!--THIS IS IMPORTANT! to close the modal, the class name has to match the name given on the ID -->
        <div id="btn-close-modal" class="close-animatedModal04 close-popup-modal"> <i class="ion-close-round"></i> </div>
        <div class="clearfix"></div>
        <div class="modal-content">
            <div class="container">
                <div class="portfolio-padding">
                    <div class="col-md-8 col-md-offset-2">
                        <h2>Sentiment Analysis of Products on Amazon by Using Deep Learning<br /></h2>
                        <h3>AI - Machine Learning - Deep Learning<br /></h3>
                        <div class="h-50"></div>
                        <p><strong>My Role</strong> </p>
                        <p>I was responsible for using an effective methodology to investigate the advantages and disadvantages of competitors’ products on Amazon. The results would help the marketing define the long-term roadmaps and short-term strategies, help the high management and product team to develop new products. </p>
                        <p>The project methodology and technical details are demonstrated below, however, the explicit data and results about the product and their competitions are moved for the purpose of business privacy protection. </p>
                        <br />
                        <p><strong>Introduction</strong> </p>
                        <p>As e-commerce has been more and more popular since this couple of decades and many consumers’ on-line shopping requirements have been raising, e-commerce platforms such as Amazon and eBay have accomplished great success and opportunities. Meanwhile, the competition of product sales and marketing in the industry is fiercer, especially for consuming products. In this background, the necessity to understand the customers’ requirements will be more and more important before improving the product quality and cutting down the product price.  Among the marketing research approaches, listening to user’s voice, e.g., researching on users’ comments and complains on that product would help the product manager, designer and other stakeholders to understand user’s real needs, the products’ pain points from a large scale more efficiently and accurately, rather than usability testing that has very small participant sample, or survey that could be more time and cost-consuming.</p>
                        <p>Sentiment analysis is also called opinion mining or emotion AI. It identifies, extracts and quantifies affective states and subjective information systematically by deploying techniques such as statistics, language processing, text analysis, computational linguistics, and biometrics. It is a classification process for analyzing textual data about the emotional statement such as positive, negative and neutral. Sentiment Analysis is also one of the more advanced tasks in natural language processing. The ultimate goal of this task is to make the computer understand the human emotional world.</p>
                        <p>Latent Dirichlet Allocation (LDA) is a topic model that can present the topic in each document in the document set based on the probability distribution. It is one of the methods of sentiment analysis. It can read the general tendency in certain topics that could be classified as regards positive and negative sentiment. The advantage of LDA model is that there is no need to manually label the training set during training, all that is needed is the document set and the number k of specified topics. In addition, another advantage of LDA is that for each topic, some words can be found to describe it.</p>
                        <br />
                        <p><strong>Solution and Technical Affordance</strong> </p>
                        <p>The picture below shows the process in this project. First, I found the product review from Amazon by data crawling programs in Python or a few existing tools. For conducting sentiment analysis, the reviews were separate into 3 categories, that were positive, negative and neutral. The “4 Star” and “5 Star” reviews were merged together as positive reviews, while “1 Star” and “2 Star” reviews were merged together as negative reviews, and the “3 Star” reviews were neutral. After the essential data formatting, I deleted unnecessary punctuations, applied tokenization, cleaning, removing stop words, stemming and lemmatization to pre-process the text. I applied LDA to conduct feature extraction, topic modeling and vectoring, then applied Word2Vec to obtain the word vectors. After those, I combined two kinds of vector matrix and imported the combination into convolutional neural network (CNN). At last, I calculated the accuracy, precision, recall and F-measure for the model with different classifications. And demonstrated the results of the sentiment analysis based on the top 10 topics in each product categories. </p>
                        <br /> <img src="img/portfolio/ai02.jpg" alt="" class="img-responsive" />
                        <br />
                        <br />
                        <p>In this project, there were 3 major technical barriers. The first was topic modelling and clustering by applying LDA model. Topic model is a statistical model used to discover abstract topics in a series of documents. Intuitively speaking, if an article has a central idea, there must be some specific words that appear more frequently. However, in reality, an article usually contains multiple topics, and the proportion of each topic is different. For example, 10% of an article is related to Apple Inc., and 90% is related to Microsoft Corporation, so it is related to Microsoft Keywords should appear 9 times more often than Apple keywords. The topic model is an automatic analysis of each document, counting the words in the document, and judging which topics the current document contains and how much each topic accounts for according to the statistical information.</p>
                        <br /> <img src="img/portfolio/ai03.jpg" alt="" class="img-responsive" />
                        <br />
                        <p>The second was Word2Vec, a toolkit for obtaining word vectors that Google open sourced in 2013. It is simple and efficient and has attracted a lot of attention. Word2vec can quickly and efficiently express a word into a vector through an optimized training model based on a given corpus, providing a new tool for applied research in the field of natural language processing. The word2vec word vector can better express the similarity and analogous relationship between different words. It has been widely used in natural language processing tasks since it was proposed, and it has become a prerequisite for a powerful natural language processing task.</p>
                        <br /> <img src="img/portfolio/ai04.jpg" alt="" class="img-responsive" />
                        <br />
                        <p>Third, I applied the CNN model similar to Kim’s structure proposed for text analysis [1]. He utilized filters with different lengths to convolve the text matrix. The width of the filter is equal to the length of the word vector. Then he used max-pooling to operate on the vectors extracted by each filter, therefore each filter reflects to a number. If stitching these filters together, we could get a vector that represents the whole sentence. The final predictions are all based on this sentence. This model has served as a classic model for a few years and has been evaluated effectively in many research papers [2].</p>
                        <br /> <img src="img/portfolio/ai05.jpg" alt="" class="img-responsive" />
                        <br />
                        <br />
                        <p><strong>Evaluation and Results</strong> </p>
                        <p>When doing classification in machine learning, we utilize some metrics to evaluate the advantages and disadvantages of algorithms, such as operational analysis, efficiency analysis, etc. The major functional analysis includes the accuracy, loss, precision, recall, F1, AUC, ROC curve, etc. </p>
                        <p>The comparison table with actual values of training and testing sets are shown below: </p>
                        <br /> <img src="img/portfolio/ai06.jpg" alt="" class="img-responsive" />
                        <br />
                        <p>In the result, the polarity of sentiments based on the top 10 topics in each product categorie, especially the competitors’ products were analyzed. All the word clouds and their sentiments of each product were also visualized to reveal their great features and pain points. The results benefited many stakeholders in the company including the marketing team and the product team. </p>
                        <br />
                        <br />
                        <br />
                        <p><strong>References</strong> </p>
                        <ol>
                        <li>Kim, Yoon. "Convolutional neural networks for sentence classification." arXiv preprint arXiv:1408.5882 (2014).</li>
                        <li>[1] Sun, Fanke, and Heping Chen. "Feature extension for chinese short text classification based on LDA and word2vec." In 2018 13th IEEE Conference on Industrial Electronics and Applications (ICIEA), pp. 1189-1194. IEEE, 2018.</li>
                        </ol>
                        <br /> </div>
                </div>
            </div>
        </div>
    </div>

<!--DEMO05-->
    <div id="animatedModal05" class="popup-modal">
        <!--THIS IS IMPORTANT! to close the modal, the class name has to match the name given on the ID -->
        <div id="btn-close-modal" class="close-animatedModal05 close-popup-modal"> <i class="ion-close-round"></i> </div>
        <div class="clearfix"></div>
        <div class="modal-content">
            <div class="container">
                <div class="portfolio-padding">
                    <div class="col-md-8 col-md-offset-2">
                        <h2>Eye-Based Selection and Navigation in Virtual Reality<br /></h2>
                        <h3>VR/AR & Empirical Study<br /></h3>
                        <div class="h-50"></div>
                        <p><strong>Introduction & Motivation</strong> </p>
                        <p>FOVE HMD was the first commercial VR handset with eye-tracking on January 2017. In order to evaluate the efficiency of the eye-tracking selecting performance in FOVE, I conducted an experiment based on the international standard, ISO 9241-9, which utilizes Fitts’ law to evaluate pointing devices. I compared three different selection techniques using the FOVE HMD: 1) eye-based selection without head-tracking, which I dub eye-only selection, 2) head-based selection without eye-tracking, dubbed head-only selection, and 3) eye-tracking and head-tracking enabled at the same time, henceforth eye+head selection. I compared these selection techniques across several different combinations of target size and depth, based on past work in 3D selection. </p>
                        <br /> <img src="img/portfolio/vr02.jpg" alt="" class="img-responsive" />
                        <br />
                        <br />
                        <p><strong>My Role</strong> </p>
                        <p>I was the first author of this study. I researched many related works, designed the experiment, developed the software, conducted the experiment and analyzed the data. My supervisor provided me with FOVE HMD, along with many ideas of experiment design and advice in the whole research project.</p>
                        <br /> 
                        <br />
                        <p><strong>My Challenge</strong> </p>
                        <p>I encountered many challenges in this project. The first was that was my first time to use FOVE, a VR HMD with eye-tracker. Before start developing, I tried out several traditional ye-trackers including Tobii, Eyetribe. I downloaded FOVE's plugin and examples then tried to change the examples in Unity and C#. The second challenge was that no studies had used 3D Fitts' law to implement and measure eye-based and head-based selections. After several versions of the software, I was finally able to get the right result.</p>
                        <br /> <img src="img/portfolio/vr05.jpg" alt="" class="img-responsive" />
                        <br />
                        <br />
                        <p><strong>Results</strong> </p>
                        <p>We found that the head-only selection offered the fastest selection times and the best accuracy. Moreover, it was strongly preferred by participants. The combination of eye-tracking and head-based selection (our eye+head input method) performed roughly between the other two, failing to leverage the benefits of each. The results indicate that, at least for the time being and in the absence of more precise eye trackers with better calibration methods, head-only selection is likely to continue to dominate VR interaction.</p>
                        <p><strong>This paper was published and presented in ACM SUI (Brighton, UK), and was elected in “Highlights of SUI”. I was then invited to present it in ACM ISS’17. This paper has been highly cited for many times by formal journals and conferences since Nov 2017.</strong> </p>
                        <br />
                        <br />
                        <br />
                        <p><strong>The Follow-up Study:  Navigation by Eyes in VR</strong> </p>
                        <p>After the selection study, I developed and conducted the navigation study in VR by eye-based technique. A benefit of eye-based navigation in VR is that it may require less physical effort compared to other input devices (e.g., wands) to explore the envoronment and move between locations, especially in large three-dimensional spaces.</p>
                        <br /> <img src="img/portfolio/vr06.jpg" alt="" class="img-responsive" />
                        <br />
                        <p><strong>This paper was published by the 6th ACM Symposium on Spatial User Interaction (SUI 2018), in Berlin, Germany. It was also chosen as one of the top papers and invited to present my work as a poster in UIST 2018 (31st ACM User Interface Software and Technology Symposium).</strong> </p>
                        <br />
                        <br /> </div>
                </div>
            </div>
        </div>
    </div>

    <!--DEMO06-->
    <div id="animatedModal06" class="popup-modal">
        <!--THIS IS IMPORTANT! to close the modal, the class name has to match the name given on the ID -->
        <div id="btn-close-modal" class="close-animatedModal06 close-popup-modal"> <i class="ion-close-round"></i> </div>
        <div class="clearfix"></div>
        <div class="modal-content">
            <div class="container">
                <div class="portfolio-padding">
                    <div class="col-md-8 col-md-offset-2">
                        <h2>Smart Safety Helmet and Wristbands for Cyclists<br /></h2>
                        <h3>IoT & Empirical Study<br /></h3>
                        <div class="h-50"></div>
                        <p><strong>Introduction & Motivation</strong> </p>
                        <p>I found many cyclists have been suffering injuries on the roads in Canada every year. Cyclists are probably pedaling in a dangerous situation but they have not realized it. Because I was hit by a motorcycle when riding a bike and then suffered two surgeries many years ago, I was strongly worried about the situation that cyclists were facing. </p>
                        <p>From previous researches, major reasons behind bicycle collisions include inattentive riding/driving, riding too close to motor vehicles, lane merging, and riding too fast. Therefore, this year, I proposed the design of a smart safety helmet that alerts cyclists from potentially dangerous situations. This smart safety helmet prototype was linked to two accompanying wristbands. When the smart safety helmet senses an approaching object, the safety wristbands alert the user in response.</p>
                        <br />
                        <p><strong>My Role</strong> </p>
                        <p>I was the idea initiator, prototype designer, hardware and software developer, statistics analysis and coordinator of user research and experiment hosting. Our team had 4 members.</p>
                        <br /> <img src="img/portfolio/helmet2.jpg" alt="" class="img-responsive" />
                        <br />
                        <p><strong>My Challenge and Design Process</strong> </p>
                        <p>It was the first time that I used Arduino to develop prototypes. I took this responsibility because the other three team members didn't have the electrical engineering background. I researched and finally developed the entire prototype system successfully.</p>
                        <p>The other challenge was that we were mainly approaching the project from a human-computer interaction perspective to evaluate the effectiveness of different types of feedback in a screen-free interface. We brainstormed and discussed, finally confirmed that we would test haptic, auditory, and visual feedback to the cyclist through the smart wristbands for exploring the most effective notification of awareness of direction and cyclist's turning decision.</p>
                        <br /> <img src="img/portfolio/helmet3.jpg" alt="" class="img-responsive" />
                        <br />
                        <br />
                        <br />
                        <p><strong>Results</strong> </p>
                        <p>We discovered that the visual notification should be the most effective, and the combination of audio and visual might be a good alternative. The haptic notification could cause misunderstanding and neglect.</p>
                        <br /> <img src="img/portfolio/helmet4.jpg" alt="" class="img-responsive" />
                        <br /> <img src="img/portfolio/helmet5.jpg" alt="" class="img-responsive" />
                        <br />
                        <p><strong>This study was published as a poster presented in the First Annual Symposium of CLUE in Ottawa, Canada.</strong></p>
                        <br />
                        <br /> </div>
                </div>
            </div>
        </div>
    </div>

 

    <!-- jQuery -->
    <script src="js/jquery.min.js"></script>
    <script src="js/jquery.js"></script>
    <!--  plugins  -->
    <script src="js/bootstrap.min.js"></script>
    <script src="js/plugins.js"></script>
    <script src="js/aos.js"></script>
    <script src="js/jquery.form.js"></script>
    <script src="js/jquery.validate.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/bars.js"></script>

    <!--  main script  -->
    <script src="js/custom.js"></script>
</body>

</html>